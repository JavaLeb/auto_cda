# ===================================================================================================================
# 数据源配置.
# ===================================================================================================================
data_source:
  type: file #数据源类型
  format: txt # 文件的格式（支持excel、txt、csv）
  field_sep: ',' # 字段（属性、特征）之间的分隔符.
  #    train_path: data\sina\sina_train.csv
  #  train_path: data/zhengqi/zhengqi_train.txt
  train_path: data\housing\housing.csv
  test_path: data\housing\housing.csv
  #  train_path: data\sina\sina_train.csv
  #  test_path: data\sina\sina_test_X.csv
  #  path: ../data/工业蒸汽量预测/data/zhengqi_train.txt  # 文件路径.
  header: 0 # 标题行.

# ===================================================================================================================
# 全局变量配置.
# target_field:
#   目标字段，或称为标签、输出字段等.
# class_fields:
#   类别字段配置. 可分为：数值型类别字段（例如：1、0）、文本型类别字段（例如：男性、女性）.
# class_text_fields:
#   文本型类别字段. 无需配置.
# class_value_fields:
#   数值型类别字段. 无需配置 class_value_fields = class_fields - class_text_fields.
# ===================================================================================================================
global:
  target_field: population
  class_fields: [ ]
  class_text_fields: [ ]

# ===================================================================================================================
# 数据探索配置.
#   head_num:
#       打印数据的前多少行.
#   field_unique_ratio:
#       字段唯一值个数占总行数的比例，如果未配置class_fields，占比不大于该值的字段被认定为是类别字段.
#   field_unique_num
#       字段唯一值个数，如果未配置class_fields，占比不大于该值的字段被认定为是类别字段.
#   explore_hist:
#       是否探索数据的直方图分布，取值true和false.
#   hist_plot_fields:
#       探索直方图的字段，未配置默认全部字段.
#   explore_relation
#       是否探索数据的相关性，数值型字段为两两相关性，取值true和false.
# ===================================================================================================================
data_explorer:

  head_num: 10

  field_unique_ratio: 0.001
  field_unique_num: 50

  show_hist_qq: false
  hist_qq_plot_fields: [ ]

  show_box: false
  box_plot_fields: [ ]

  show_relation: true
  relation_threshold: 0.5

  duplicate_fields: [ ]


# ===================================================================================================================
# 数据切分器配置
# splitter:
#   配置数据切分器，目前支持simple、KFold、LeaveOneOut、LeavePOut.
#   simple：简单交叉验证.
#   KFold：K折交叉验证.
#   StratifiedKFold：分层K折交叉验证.
#   LeaveOneOut：留一法交叉验证.
#   LeavePOut：留P法交叉验证.
# params:
#   配置切分器参数，例如：train_size: 0.7, test_size: 0.3.
#   simple: 可配置参数：train_size=0.7, test_size=0.4, random_state=0, shuffle=False.
#           train_size也可以配置整数，表示样本个数.
#   KFold: 可配置参数：n_splits=5, shuffle=False.
#   StratifiedKFold：可配置参数：n_splits=5, shuffle=False.
#   LeavePOut：可配置参数：p=1.
# ===================================================================================================================
data_splitter:
  splitter: simple
  params:
    train_size: 0.8
    shuffle: True
    random_state: 42


# ===================================================================================================================
# 数据处理器配置
# ===================================================================================================================
data_processor:
  # ---------------------------------------------------------------------------------------------------------------
  # 字段选择器配置：
  #   对字段进行选择，例如：删除一些不需要的字段.
  #   字段选择的方法：
  #     1、方差阈值化：
  #           （1）如果特征字段是数值型，可计算该字段的方差，如果方差很小，低于设定的阈值，表示该字段重要性很低，可删除.
  #           （2）如果特征字段是二元类别，方差计算var(x)=p(1-p).
  #           注意：不可事先做中心化、标准化等.
  #     2、利用统计检验，目标字段是类别型（分类问题）：
  #           （1）输入字段是数值型：卡方检验.
  #           （2）输入字段是类别型：ANOVA检验和T检验.
  #     3、相关性系数，目标字段和非目标字段都是数值型.
  #     4、模型的方式：使用决策树、随机森林、XGBoost以及逻辑回归中的逐步回归.
  # fields:
  #   配置需要删除的字段名称，多个字段使用英文逗号","分隔，例如[f1,f2,f3].
  # ---------------------------------------------------------------------------------------------------------------
  field_selection:
    fields: [ V5, V9, V11, V17, V22, V28 ]
    variance_threshold_selection:
      - fields: [ longitude ]
        threshold: 1000000
      - fields: [ ]
        threshold:
  # ---------------------------------------------------------------------------------------------------------------
  # 字段清洗器配置. 例如：对字段缺失值进行填充.
  #   na_cleaner:
  #     对字段缺失值清洗配置. 一般缺失值处理有如下方法：
  #       1、缺失值比例 < 20% :
  #           对字段值进行填补：
  #             类别型字段：（1）使用常数、众数填充，
  #                       （2）使用KNN、随机森林、XGBoost分类模型填充；
  #             数值型字段：（1）使用常数、平均值、中位数填充，
  #                       （2）使用KNN、随机森林、XGBoost回归模型填充；
  #       2、缺失值比例  20%~80% :
  #           对字段值进行填补，填补方法同上，同时新增指示变量，非缺失值标记为1，缺失值标记为0.
  #       3、缺失值比例  >80% :
  #           删除缺失字段，同时新增指示变量，非缺失值标记为1，缺失值标记为0.
  #       4、直接删除字段缺失值记录：
  #            （1）一般总数据量很大，该字段缺失值只占一小部分时，可直接删除，此种方法使用较少；
  #       代码实现时暂不支持第3条.
  #     fields:
  #       配置需要缺失值清洗的字段名称，多个字段使用英文逗号","分隔，例如：fields: [f1,f2,f3].
  #     clean_method:
  #       配置具体的清洗方法.
  #         drop: 直接删除缺失值字段；
  #         drop_na: 直接删除缺失值记录；
  #         simple_fill: 缺失值简单填补；
  #         knn_fill: 最近邻填充，支持数值型字段.
  #         rfc_fill: 随机森林分类填充，支持类别型字段.
  #         rfr_fill: 随机森林回归填充，支持数值型字段.
  #     fill_params:
  #       配置填补的参数，
  #      （1）当 clean_method = simple_fill时. 可配置参数：{ strategy: ,fill_value:, add_indicator: }, sklearn.impute.SimpleImputer.
  #           strategy:
  #             constant: 常数填补.
  #             mean: 均值填补.
  #             median: 中位数填补.
  #             most_frequent: 众数填补.
  #           fill_value: 填充的常数值，当 strategy = constant 时生效.
  #           add_indicate:
  #               是否新增指示变量，配置取值true or false；指示变量的取值策略：缺失值为1，非缺失值为0.
  #      （2）当clean_method = knn_fill时，可配置参数参见sklearn.impute.KNNImputer.
  # ---------------------------------------------------------------------------------------------------------------
  field_cleaner:
    na_cleaners:
      - fields: [ ocean_proximity ]
        clean_method: rfc_fill
  #        fill_params: { strategy: mean ,fill_value:, add_indicator: true}

  # ---------------------------------------------------------------------------------------------------------------
  # 字段转换器配置.
  #   对字段进行转换， 一个字段如果配置了多个转换器，按照配置顺序执行.
  #   转换器可以将一个字段变为多个字段，例如独特编码，新字段命名方式为：原字段名称_序号.
  #   如果一个字段通过某个转换器变成多个字段，后续的转换器将作用于多个字段.
  #   转换仅支持字段的值依赖当前的列.
  #   特征的转换方法：
  #     1、线性特征转换：
  #         （1）与目标字段无关的转换：PCA、SVD、TSVD、矩阵分解NMF，
  #         （2）与目标字段相关的转换：LDA.
  #     2、非线性特征转换：
  #         （1）与目标字段无关的转换：Kenerl PCA、t-SNE
  #         （2）与目标字段有关的转换：神经网络.
  # fields:
  #   配置需要进行转换的字段名称，多个字段使用英文逗号","分隔，例如[f1,f2,f3].
  # transformers:
  #   配置转换器. 数据转换器有如下种类：
  #     1、编码转换器：对字段进行编码。通常对非数值的类别型字段编码，支持：
  #         OneHotEncoder:  独特编码，主要用于无序的类别型字段；
  #         OrdinalEncoder: 有序编码，主要用于有序的类别字段编码；
  #     2、标准化转换器：对字段数值进行标准化，支持：
  #         MinMaxScaler：最大最小标准化
  #         StandardScaler：标准化.
  #     4、其他系统内置转换器：系统内置的转换器均可，例如：
  #         TfidfVectorizer：文本词频-逆文档频率.
  #     4、自定义转换器：需实现fit()、transform()
  #   name: 配置转换器名称，支持如下转换器：
  #   params: 配置转换器的参数，格式: {param1: value1,param2: value2}.
  # ---------------------------------------------------------------------------------------------------------------
  field_transformer:
    pipeline:
      steps:
        - step:
            name: pre
            column_transformer:
              transformers:
                - name: standard_scaler
                  transformer:
                    instance: StandardScaler
                    params:
                  fields: [ 'V0', 'V1' ]
                - name: min_max_scaler
                  transformer:
                    instance: MinMaxScaler
                    params:
                  fields: [ 'V2', 'V3' ]
              params:
        - step:
            name: pca
            transformer:
              instance: PCA
              params: { n_components: 0.9 }
      params:

# ===================================================================================================================
# 数据模型配置. 最佳模型：对于不同模型、不同参数，将根据评估指标选择对应的最佳模型.
#
# target_fields：
#     配置目标字段，如果未配置目标字段，数据中的最后一个字段认定为是目标字段.
# models:
#     配置数据模型，可配置多个数据模型.
#   estimator:
#        模型名称，支持SVC等各种回归、分类模型.
#   param_grid:
#        配置模型参数，每个参数可配置多个值.
# fine_tune:
#     模型调优方法，支持：GridSearchCV（网格搜索）、RandomizedSearchCV（随机搜索）
# assessments:
#     评估方法配置.支持sklearn.metrics中评估指标.
#     name:
#       评估方法名称.支持如下：
#         回归指标.
#           mean_squared_error: 均方误差.
#           root_mean_squared_error:
#         分类指标：
#           accuracy_score
#           precision_score
#           recall_score
#           f1_score
#           roc_auc_score
#           classification_report
#           confusion_matrix
#     min_max:
#       评估准则，根据评估指标的最大值还是最小值来判断最佳.
#     params:
#       评估方法的参数.
# ===================================================================================================================
data_modeler:
  target_fields: [ channel_title ]
  target_encoder:
    name: LabelEncoder
  models:
    - estimator: SVC
      param_grid: [ ]
    - estimator: RandomForestClassifier
  fine_tune: RandomizedSearchCV
  assessments:
    - name: accuracy_score
      min_max: max
      params:
    - name: precision_score
      min_max: max
      params: { average: micro }
    - name: f1_score
      min_max: max
      params: { average: micro }
    #    - name: recall_score
    #      min_max: max
    #      params: { average: micro }
  #      - name: classification_report
  #        min_max: max
  #        params:
  #    - name: roc_auc_score
  #      min_max: max
  #      params: { multi_class: ovo }
  save_predict:
    path: result/predict_result.csv
    target_name: [ channel_title ]