# ===================================================================================================================
# 全局变量配置.
# target_field:
#   目标字段，或称为标签、输出字段等.
# class_fields:
#   类别字段配置. 可分为：数值型类别字段（例如：1、0）、文本型类别字段（例如：男性、女性）.
# class_text_fields:
#   文本型类别字段. 无需配置.
# format:
#     %Y-%m-%d %H:%M:%S
# ===================================================================================================================
global:
  target_field: group
  date_field:
  class_fields: [ ]
  class_text_fields: [ ]

# ===================================================================================================================
# 数据源配置.
#     type:
#         数据源类型，支持file.
#     format:
#         文件格式，支持csv（包括txt文本），excel.
#     field_sep:
#         列分隔符， example: ','.
#     train_path:
#         训练数据路径，仅支持单个文件，多个文件使用 DataIntegration提供的read()，example: ../data/202409/training_data.csv.
#     test_path:
#         测试数据路径，仅支持单个文件，example: ../data/202409/test_data.csv.
#     header:
#         标题行，从0开始，无标题行不配置.  example: 0.
# ===================================================================================================================
data_source:
  type: file
  format: txt
  field_sep: ','
  train_path: data/202409/training_data.csv
  test_path:  data/202409/test_data.csv
  header: 0

# ===================================================================================================================
# 数据探索配置.
#   head_num:
#       打印数据的前多少行，example: 10.
#   field_unique_ratio:
#       字段唯一值个数占字段值总个数的比例阈值，占比不大于该阈值的字段被认定为是类别字段，example: 0.0001.
#   field_unique_num
#       字段唯一值个数阈值，字段唯一值个数不大于该阈值的字段被认定为是类别字段，example: 20.
#   show_hist_qq:
#       是否探索数据的直方图分布和QQ图，取值true和false，默认false， example: true.
#   hist_qq_plot_fields:
#       配置需要探索直方图的字段，未配置默认全部字段，example: [field1,field2].
#   qq_row_num:
#       一张图显示的子图行数，example: 2.
#   qq_col_num:
#       一张图显示的列子图个数，example: 1.
#   show_box:
#       是否探索箱型图，取值true和false，默认false，example: true.
#   box_plot_fields:
#       需要探索箱型图的字段，未配置默认全部字段，example: [field1,field2].
#   box_row_num:
#       一张图显示的箱型行子图的个数，example: 2.
#   box_col_num:
#       一张图显示的箱型列子图个数，example: 1.
#   show_relation:
#       是否探索字段关系，取值true和false，默认false，example: true.
#   relation_threshold:
#       字段相关性系数阈值，默认0.5，example: 0.6.
#   relation_row_num:
#       一张图显示的关系图行子图个数，example: 1.
#   relation_col_num:
#       一张图显示的关系图列子图的个数，example: 1.
#   show_compare:
#       对比探索两个数据.
# ===================================================================================================================
data_explorer:

  head_num: 10

  field_unique_ratio: 0.0001
  field_unique_num: 200

  show_hist_qq: true
  hist_qq_plot_fields: [ ]
  qq_row_num: 2
  qq_col_num: 1

  show_box: true
  box_plot_fields: [ ]
  box_row_num: 2
  box_col_num: 1

  show_relation: true
  relation_threshold: 0.5
  relation_row_num: 1
  relation_col_num: 1

  show_compare: true

# ===================================================================================================================
# 数据切分器配置
# splitter:
#   配置数据切分器，目前支持simple、KFold、LeaveOneOut、LeavePOut.
#   simple：简单交叉验证.
#   KFold：K折交叉验证.
#   StratifiedKFold：分层K折交叉验证.
#   LeaveOneOut：留一法交叉验证.
#   LeavePOut：留P法交叉验证.
# params:
#   配置切分器参数，例如：train_size: 0.7, test_size: 0.3.
#   simple: 可配置参数：train_size=0.7, test_size=0.4, random_state=0, shuffle=False.
#           train_size也可以配置整数，表示样本个数.
#   KFold: 可配置参数：n_splits=5, shuffle=False.
#   StratifiedKFold：可配置参数：n_splits=5, shuffle=False.
#   LeavePOut：可配置参数：p=1.
# ===================================================================================================================
data_splitter:
  splitter: simple
  params:
    train_size: 0.8
    shuffle: True
    random_state: 42


# ===================================================================================================================
# 数据处理器配置
# ===================================================================================================================
data_processor:
  # ---------------------------------------------------------------------------------------------------------------
  # 字段选择器配置：
  #   对字段进行选择，例如：删除一些不需要的字段.
  #   字段选择的方法：
  #     1、方差阈值化：
  #           （1）如果特征字段是数值型，可计算该字段的方差，如果方差很小，低于设定的阈值，表示该字段重要性很低，可删除.
  #           （2）如果特征字段是二元类别，方差计算var(x)=p(1-p).
  #           注意：不可事先做中心化、标准化等.
  #     2、利用统计检验，目标字段是类别型（分类问题）：
  #           （1）输入字段是数值型：卡方检验.
  #           （2）输入字段是类别型：ANOVA检验和T检验.
  #     3、相关性系数，目标字段和非目标字段都是数值型.
  #     4、模型的方式：使用决策树、随机森林、XGBoost以及逻辑回归中的逐步回归.
  # fields:
  #   配置需要删除的字段名称，多个字段使用英文逗号","分隔，例如[f1,f2,f3].
  # ---------------------------------------------------------------------------------------------------------------
  field_selection:
    drop_fields: [ device_id ]
    variance_threshold_selection:
      - fields: [ ]
        threshold: 1000000
      - fields: [ ]
        threshold:
  # ---------------------------------------------------------------------------------------------------------------
  # 字段清洗器配置. 例如：对字段缺失值进行填充.
  #   na_cleaner:
  #     对字段缺失值清洗配置. 一般缺失值处理有如下方法：
  #       1、缺失值比例 < 20% :
  #           对字段值进行填补：
  #             类别型字段：（1）使用常数、众数填充，
  #                       （2）使用KNN、随机森林、XGBoost分类模型填充；
  #             数值型字段：（1）使用常数、平均值、中位数填充，
  #                       （2）使用KNN、随机森林、XGBoost回归模型填充；
  #       2、缺失值比例  20%~80% :
  #           对字段值进行填补，填补方法同上，同时新增指示变量，非缺失值标记为1，缺失值标记为0.
  #       3、缺失值比例  >80% :
  #           删除缺失字段，同时新增指示变量，非缺失值标记为1，缺失值标记为0.
  #       4、直接删除字段缺失值记录：
  #            （1）一般总数据量很大，该字段缺失值只占一小部分时，可直接删除，此种方法使用较少；
  #       代码实现时暂不支持第3条.
  #     fields:
  #       配置需要缺失值清洗的字段名称，多个字段使用英文逗号","分隔，例如：fields: [f1,f2,f3].
  #     clean_method:
  #       配置具体的清洗方法.
  #         drop: 直接删除缺失值字段；
  #         drop_na: 直接删除缺失值记录；
  #         simple_fill: 缺失值简单填补；
  #         knn_fill: 最近邻填充，支持数值型字段.
  #         rfc_fill: 随机森林分类填充，支持类别型字段.
  #         rfr_fill: 随机森林回归填充，支持数值型字段.
  #     fill_params:
  #       配置填补的参数，
  #      （1）当 clean_method = simple_fill时. 可配置参数：{ strategy: ,fill_value:, add_indicator: }, sklearn.impute.SimpleImputer.
  #           strategy:
  #             constant: 常数填补.
  #             mean: 均值填补.
  #             median: 中位数填补.
  #             most_frequent: 众数填补.
  #           fill_value: 填充的常数值，当 strategy = constant 时生效.
  #           add_indicate:
  #               是否新增指示变量，配置取值true or false；指示变量的取值策略：缺失值为1，非缺失值为0.
  #      （2）当clean_method = knn_fill时，可配置参数参见sklearn.impute.KNNImputer.
  # ---------------------------------------------------------------------------------------------------------------
  field_cleaner:
    outlier_cleaners:
      - fields: [ ]
        encoder:
        detector: 3sigma   # 支持3sigma、IQR
        clean_method:   # 同 na_cleaners: clean_method.
      - fields: [ ]
        encoder:
        detector: 3sigma   # 支持3sigma、IQR
        clean_method:   # 同 na_cleaners: clean_method.
    na_cleaners:
      - fields: [ event_id_min,event_id_mean,event_id_max,event_id_nunique,
                  app_id_min,app_id_mean,app_id_max,app_id_nunique,
                  longitude_min,longitude_mean,longitude_max,longitude_std,
                  latitude_min,latitude_mean,latitude_max,latitude_std,timestamp_nunique,
                  is_installed_min,is_installed_mean,is_installed_max,is_installed_sum,
                  is_active_min,is_active_mean,is_active_max,is_active_sum]
        clean_method: simple_fill
        fill_params: { strategy: constant,
                       fill_value: 0,
                       add_indicator: false }
      - fields: [ timestamp_min,timestamp_max ]
        clean_method: simple_fill
        fill_params: { strategy: mean,
                       fill_value: ,
                       add_indicator: false }
      - fields: [ label_id,category ]
        clean_method: simple_fill
        fill_params: { strategy: constant,
                       fill_value: ,
                       add_indicator: false }
  # ---------------------------------------------------------------------------------------------------------------
  # 数据转换器配置.
  #   对数据进行转换， 每个数据转换器由多个step构成，一个step可以是ColumnTransformer或其他Transformer构成.
  #   ColumnTransformer由多个Transformer，只能配置在第一个step.
  #   特征的转换方法：
  #     1、线性特征转换：
  #         （1）与目标字段无关的转换：PCA、SVD、TSVD、矩阵分解NMF，
  #         （2）与目标字段相关的转换：LDA.
  #     2、非线性特征转换：
  #         （1）与目标字段无关的转换：Kenerl PCA、t-SNE
  #         （2）与目标字段有关的转换：神经网络.
  # fields:
  #   配置需要进行转换的字段名称，多个字段使用英文逗号","分隔，例如[f1,f2,f3].
  # transformers:
  #   配置转换器. 数据转换器有如下种类：
  #     1、编码转换器：对字段进行编码。通常对非数值的类别型字段编码，支持：
  #         OneHotEncoder:  独特编码，主要用于无序的类别型字段；
  #         OrdinalEncoder: 有序编码，主要用于有序的类别字段编码；
  #     2、标准化转换器：对字段数值进行标准化，支持：
  #         MinMaxScaler：最大最小标准化
  #         StandardScaler：标准化.
  #     4、其他系统内置转换器：系统内置的转换器均可，例如：
  #         TfidfVectorizer：文本词频-逆文档频率.
  #     4、自定义转换器：需实现fit()、transform()
  #   name: 配置转换器名称，支持如下转换器：
  #   params: 配置转换器的参数，格式: {param1: value1,param2: value2}.
  # ---------------------------------------------------------------------------------------------------------------
  data_transformer:
    steps:
      - step_name: ColumnTransformer
        column_transformer:
          transformers:
            - transformer_name: OrdinalEncoder
              transformer:
                instance: OrdinalEncoder
              fields: [ phone_brand,device_model ]
              fields_form: list
            - transformer_name: TfidfVectorizer
              transformer:
                instance: TfidfVectorizer
                params: {max_features: 50}
              fields: [ label_id,category ]
          params:
      - step_name: MaxAbsScaler
        transformer:
          instance: MaxAbsScaler
          params:



#      - step_name: pre
#        column_transformer:
#          transformers:
#            - transformer_name: min_max_scaler
#              transformer:
#                instance: MinMaxScaler
#              #                params: { feature_range: [0,1] }
#              fields: [ '*' ]
#          params:

#      - step_name: box_cox
#        transformer:
#          instance: BoxCoxTransformer
#          params:

#      - step_name: pca
#        transformer:
#          instance: PCA
#          params: { n_components: 0.9 }

# ===================================================================================================================
# 数据模型配置. 最佳模型：对于不同模型、不同参数，将根据评估指标选择对应的最佳模型.
#
# target_fields：
#     配置目标字段，如果未配置目标字段，数据中的最后一个字段认定为是目标字段.
# models:
#     配置数据模型，可配置多个数据模型.
#   estimator:
#        模型名称支持SVC等各种回归、分类模型.
#           回归:
#                    线性回归: sklearn.linear_model.LinearRegression
#                  K-近邻回归: sklearn.neighbors.KNeighborsRegressor
#                                param_grid: {n_neighbors: 3}
#               支持向量机回归: sklearn.svm.SVR
#                  决策树回归: sklearn.tree.DecisionTreeRegressor
#                随机森林回归: sklearn.ensemble.RandomForestRegressor
#                                param_grid: {n_estimators: [100,300,500], max_depth: [1,5,10,20]}
#               LightGBM回归: lightgbm.LGBMRegressor
#                          : sklearn.ensemble.BaggingRegressor
#        分类:
#               集成学习: sklearn.ensemble.RandomForestClassifier
#                        sklearn.ensemble.BaggingClassifier
#                        sklearn.ensemble.AdaBoostClassifier
#                        sklearn.ensemble.VotingClassifier
#                        sklearn.ensemble.StackingClassifier
#                        sklearn.ensemble.GradientBoostingClassifier
#               线性模型： sklearn.linear_model.LogisticRegression
#             支持向量机:  sklearn.svm.SVC
#                决策树:  sklearn.tree.DecisionTreeClassifier
#                最近邻:  sklearn.neighbors.KNeighborsClassifier
#             lightgbm:  lightgbm.LGBMClassifier
#           随机梯度下降:  sklearn.linear_model.SGDClassifier
#              xgboost:  xgboost.XGBClassifier
#                        xgboost.XGBRFClassifier
#   param_grid:
#        配置模型参数，每个参数可配置多个值.
# fine_tune:
#     模型调优方法，支持：GridSearchCV（网格搜索）
# assessments:
#     评估方法配置.支持sklearn.metrics中评估指标.
#     name:
#       评估方法名称.支持如下：
#         回归指标:
#            平均绝对值误差: sklearn.metrics.mean_absolute_error
#                 均方误差: sklearn.metrics.mean_squared_error
#               均方根误差: sklearn.metrics.root_mean_squared_error
#                 R平方值: sklearn.metrics.r2_score
#         分类指标：
#                  准确率: sklearn.metrics.accuracy_score
#                  精准度: sklearn.metrics.precision_score
#                  召回率: sklearn.metrics.recall_score
#                  f1得分: sklearn.metrics.f1_score
#                 AUC得分: sklearn.metrics.roc_auc_score
#                 分类报告: sklearn.metrics.classification_report
#                 混淆矩阵: sklearn.metrics.confusion_matrix
#     min_max:
#       评估准则，根据评估指标的最大值还是最小值来判断最佳.
#     params:
#       评估方法的参数.
# ===================================================================================================================
data_modeler:
  save_path: result/model
  target_encoder:
    name: LabelEncoder
  models:
    # 分类.
    #    - estimator: sklearn.ensemble.AdaBoostClassifier
    #    - estimator: sklearn.svm.SVC
    #    - estimator: sklearn.ensemble.BaggingClassifier
    #    - estimator: xgboost.XGBClassifier
    - estimator: sklearn.linear_model.LogisticRegression
    # 回归
  #    - estimator: sklearn.linear_model.LinearRegression
  #      param_grid: [ ]
  #    - estimator: sklearn.neighbors.KNeighborsRegressor
  #    - estimator: sklearn.svm.SVR
  #    - estimator: sklearn.tree.DecisionTreeRegressor
  #    - estimator: sklearn.ensemble.RandomForestRegressor
  #    - estimator: lightgbm.LGBMRegressor
  #    - estimator: RandomForestClassifier
  fine_tune: GridSearchCV
  assessments:
    #    - name: mean_squared_error
    #      min_max: min
    #    - name: r2_score
    #      min_max: max
    #    - name: accuracy_score
    #      min_max: max
    #      params:
    #    - name: precision_score
    #      min_max: max
    #      params: { average: micro }
    - name: f1_score
      min_max: max
      params: { average: micro }
  #    - name: recall_score
  #      min_max: max
  #      params: { average: micro }
  #    - name: classification_report
  #      min_max: max
  #      params:
  #    - name: roc_auc_score
  #      min_max: max
  #      params: { multi_class: ovo }

# ===================================================================================================================
# 数据结果提交配置.
# result_file_dir:
#   结果文件目录.
# result_file_name:
#   结果文件名称.
# include_field:
#   保存字段列表，未配置表示保存所有字段.
# exclude_field:
#   不保存的字段列表.
# predict_field:
#   预测字段名称.
# predict_field_position:
#   预测字段在结果中的列索引，整数类型，例如：-1表示将预测字段保存在最后一列.
# ===================================================================================================================
data_submission:
  result_file_dir: result/submission
  result_file_name: 考生姓名_results.csv
  save_field: []
  exclude_field: []
  predict_field: Predicted_Results
  predict_field_position: -1
